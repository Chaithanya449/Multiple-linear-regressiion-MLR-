# -*- coding: utf-8 -*-
"""MLR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W5bdY0setKc7X61zovdyuibB9ud9I8qg
"""

# Importing the required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Loading the data
df = pd.read_csv('ToyotaCorolla_MLR.csv')
df.head()

df.tail()

df.info()

object_cols=df.select_dtypes(include = 'object').columns
number_cols=df.select_dtypes(include = 'number').columns  # Its clear that all the datatypes are correct and no need to convert one data type to another.
print(object_cols)
print(number_cols)

# Identifying the null values
df.isnull().sum() # The dataframe has no null values so there would be no filling of null values using imputation methods

# Getting the stats of dataset  using describe
df.describe()

# Detecting the outliers using boxplot
numeric_cols = ['Price', 'Age_08_04', 'KM', 'HP','cc', 'Doors','Cylinders','Gears', 'Weight']
for col in numeric_cols:
   plt.figure(figsize=(5,6))
   sns.boxplot(y = df[col])
   plt.title(col)
   plt.tight_layout()
   plt.show()

# Using log scale to observe outliers in cc column as normal boxplot showing flat shape
plt.figure(figsize=(5,6))
sns.boxplot(y = np.log(df['cc']))
plt.title('cc')
plt.tight_layout()
plt.show()

"""Boxplot of gears and cylinders is tightly packed as there might be lesser values a boxplot requires large values. Also the values in gears are 3,4,5 and cylinders values are 4,6,8 which are unique values so boxplot fails to show distribution."""

sns.countplot(x='Automatic', data=df) # Automatic is binary data 0 and 1 results in yes/no hence applying barplot as boxplot only gives flat and tightly packed shape almost invisible
plt.title("Transmission Type Count")
plt.xticks([0, 1], ['Manual', 'Automatic'],rotation = 0) # Note: boxplot is only for numeric continuous data.Using boxplot binary data will not show distribution
plt.show()

sns.countplot(x = 'Fuel_Type',data = df)
plt.title('Fuel type counts')
plt.xticks(rotation = 45)
plt.tight_layout()
plt.show()

# Removing outliers using winsorization capping method
from scipy.stats.mstats import winsorize
winsorized_df = df.copy()
for col in numeric_cols:  # for loop is used as winsorization can only be applied to one column to apply winsorization for multiple columns for loop is implemented
    df[col] = winsorize(df[col],limits = [0.05,0.05])
    winsorized_df[col] = df[col]

n = len(numeric_cols)
fig, axes = plt.subplots(nrows=n, ncols=2, figsize=(10, 4*n))
fig.suptitle('Boxplots Before and After Winsorization', fontsize=16)

for i, col in enumerate(numeric_cols):
    sns.boxplot(x=df[col], ax=axes[i, 0], color='lightcoral')
    axes[i, 0].set_title(col + ' - Original')
    sns.boxplot(x=winsorized_df[col], ax=axes[i, 1], color='lightgreen')
    axes[i, 1].set_title(col + ' - Winsorized')

plt.tight_layout(rect=[0, 0.03, 1, 0.98])
plt.show()

"""As we can see the outliers are not completely removed so we use Winsorization IQR method.After the data is winsorized IQR method is applied onto the winsorized data."""

# Applying IQR-based winsorization to the winsorized_df to remove the outliers completely
import warnings
warnings.filterwarnings('ignore')
numeric_cols = winsorized_df.select_dtypes(include='number').columns.tolist()

# Creating a copy of winsorized_df
winsorized_iqr_df = winsorized_df.copy()

for col in numeric_cols:
    Q1 = winsorized_df[col].quantile(0.25)
    Q3 = winsorized_df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    winsorized_iqr_df[col] = winsorized_df[col].clip(lower=lower, upper=upper)

#  comparing  5%-winsorized on left vs IQR-winsorized on right
fig, axes = plt.subplots(len(numeric_cols), 2, figsize=(10, 4*len(numeric_cols)))
fig.suptitle('5% Winsorized vs IQR Winsorized', fontsize=16)

for idx, col in enumerate(numeric_cols):
    sns.boxplot(x=winsorized_df[col], ax=axes[idx, 0], color='orange')
    axes[idx, 0].set_title(col + ' - 5% Winsor')
    sns.boxplot(x=winsorized_iqr_df[col], ax=axes[idx, 1], color='lightblue')
    axes[idx, 1].set_title(col + ' - IQR Winsor')

plt.tight_layout(rect=[0, 0.03, 1, 0.98])
plt.show()



#Obtaining stats from the numerical cols and creating a dataframe
mean = winsorized_iqr_df[numeric_cols].mean()
median = winsorized_iqr_df[numeric_cols].median()
mode = winsorized_iqr_df[numeric_cols].mode().iloc[0]
min_value = winsorized_iqr_df[numeric_cols].min()
max_value = winsorized_iqr_df[numeric_cols].max()
std = winsorized_iqr_df[numeric_cols].std()
summary_stats = pd.DataFrame({
    'Mean': mean,
    'Median': median,
    'Mode': mode,
    'Min': min_value,
    'Max': max_value
})
print(summary_stats)

"""Key insights from summary_stats:
The cars in this dataset mostly cost around ₹10,500, with prices ranging from ₹6,900 to ₹17,200. On average, they are about 4 to 5 years old and have been driven nearly 67,000 kilometers. Most cars have around 100 horsepower and are all manual—no automatic cars here. The engine size is usually 1600 cc, and the number of doors is mostly 4, though a few have 3 or 5. Every car comes with 4 cylinders and 5 gears, and they typically weigh around 1070 kilograms.


"""

#Importing essential libraries to build MLR model
import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
# Using pd.get_dummies to convert categorical data to numerical as ML models require the data in numerical
df_2= pd.get_dummies(winsorized_iqr_df , columns = ['Fuel_Type'],drop_first = True) # Using pd.get_dummies to convert categorical

# Determing the features for target and independent features
X_1 = df_2[['Age_08_04', 'KM', 'HP', 'Automatic']] # model will be built with only 4 features
Y_1 =df_2 ['Price']

# Splitting the data using train_test_split
x_1train,x_1test,y_1train,y_1test = train_test_split(X_1,Y_1,test_size = 0.2,random_state = 42)

# Building the model 1
model_1 = LinearRegression()
model_1.fit(x_1train,y_1train)

# Building the Model 2
X_2= df_2[['Age_08_04', 'KM', 'HP', 'Automatic','Fuel_Type_Diesel','Fuel_Type_Petrol']]# For this model extra Fuel_type is added as a X feature

'''After getting dummies from Fuel_type the dataframe will create a new columns in dataframe as Fuel_Type_Diesel and Fuel_Type_petrol.
when column name as Fuel_Type is given as feature it will throw an error as  Fuel_Type does not exist in the dataframe '''

Y_2 = df_2['Price']

x_2train,x_2test,y_2train,y_2test = train_test_split(X_2,Y_2,test_size = 0.2,random_state = 42)
model_2 = LinearRegression()
model_2.fit(x_2train,y_2train)

#Building the model 3   # Building the  model with all the features
X_3 = df_2[['Age_08_04', 'KM', 'HP', 'Automatic', 'cc', 'Doors', 'Cylinders', 'Gears', 'Weight','Fuel_Type_Diesel','Fuel_Type_Petrol']]
Y_3  = df_2['Price']
x_3train,x_3test,y_3train,y_3test = train_test_split(X_3,Y_3,test_size = 0.2 ,random_state = 42)
model_3 = LinearRegression()
model_3.fit(x_3train,y_3train)

import numpy as np
np.set_printoptions(precision=2, suppress=True)
print(f'm1_coef:{model_1.coef_}')
print(f'm2_intercept:{model_1.intercept_:.2f}')
print(model_1.score(x_1test,y_1test))
print(f'm2_coef:{model_2.coef_}:.2f')
print(f'm2_intercept:{model_2.intercept_:.2f}')
print(model_2.score(x_2test,y_2test))
print(f'm3_coef:{model_3.coef_}:.2f')
print(f'm3_intercept:{model_3.intercept_:.2f}')
print(model_3.score(x_3test,y_3test))

"""Lets understand the role of coefficients in the model 3 as it contains all the features and its accuracy is higher almost 87%. The more the weight the more the price.Again weight is positive and negative.Fuel type is having a weight of +1079 which implies that it is an important feature and also that means for every car if fuel is diesel then  +1079 is added.we can say that fuel type  is important feature and finally it will increase the car price the same is with the fuel type petrol which has the weight of 1345.Age is having weight of -111 which means it is an important feature in lowering the price of the car.More the age of car in months the less is its price. -0.1 is the weight of km that means the more km a car drove the less is its price.
Note:The more the weight the more the car price but here we also need to consider the positive and negative weight.positive weight will increase the car price wheras negative will decrease the car price.  
"""

# Model evaluation
from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
# model evaluation for model_1
predictions_1 = model_1.predict(x_1test)
print("--- Model 1 Evaluation ---")
mae = mean_absolute_error(y_1test, predictions_1)
mse = mean_squared_error(y_1test, predictions_1)
r2 = r2_score(y_1test, predictions_1)

print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R-squared (R2 Score): {r2:.3f}")
predictions_2 = model_2.predict(x_2test)
print("--- Model 2 Evaluation ---")
mae = mean_absolute_error(y_2test, predictions_2)
mse = mean_squared_error(y_2test, predictions_2)
r2 = r2_score(y_2test, predictions_2)
print(f'Mean Absolute Error (MAE): {mae:.2f}')
print(f'Mean Squared Error (MSE): {mse:.2f}')
print(f'R-squared (R2 Score): {r2:.3f}')
predictions_3 = model_3.predict(x_3test)
print("--- Model 3 Evaluation ---")
mae = mean_absolute_error(y_3test, predictions_3)
mse = mean_squared_error(y_3test, predictions_3)
r2 = r2_score(y_3test, predictions_3)
print(f'Mean Absolute Error (MAE): {mae:.2f}')
print(f'Mean Squared Error (MSE): {mse:.2f}')
print(f'R-squared (R2 Score): {r2:.3f}')

"""The concept of overfitting:
  A model will predict poorly when it has lots of features.Lasso and ridge are techiques that are used to avoid the problem of overfitting.

"""

# Applying Ridge onto the 3 models
from sklearn.linear_model import Ridge
np.set_printoptions(precision=2, suppress=True)
ridge_model1 = Ridge(alpha = 1.0)
ridge_model1.fit(x_1train,y_1train)
print(f'ridge_model_1: {ridge_model1.score(x_1test,y_1test)}')
ridge_model2 = Ridge(alpha = 1.0)
ridge_model2.fit(x_2train,y_2train)
print(f'ridge_model_2: {ridge_model2.score(x_2test,y_2test)}')
ridge_model3 = Ridge(alpha = 1.0)
ridge_model3.fit(x_3train,y_3train)
print(f'ridge_model3 : {ridge_model3.score(x_3test,y_3test)}')

"""The r2_scores of 3 models and scores of ridge models are all same that means the model is good fit and not overfit.Though the model is not overfit the ridge still applies the penalty and the weights are made small no much difference when compared to regression.we can say ridge has only made some small adjustments and the model is not overfit.Ridge only reduces the weights of features but not remove it like in the lasso regression."""

# Applying lasso on 3 models
from sklearn.linear_model import Lasso
lasso_model1 = Lasso(alpha = 1.0)
lasso_model1.fit(x_1train,y_1train)
print(f'lasso_model_1: {lasso_model1.score(x_1test,y_1test)}')
lasso_model2 = Lasso(alpha = 1.0)
lasso_model2.fit(x_2train,y_2train)
print(f'lasso_model_2: {lasso_model2.score(x_2test,y_2test)}')
lasso_model3 = Lasso(alpha = 1.0)
lasso_model3.fit(x_3train,y_3train)
print(f'lasso_model3 : {lasso_model3.score(x_3test,y_3test)}')

"""Lasso is a technique that removes unimportant features by making their weights as '0'.We can definitely say that lasso has removed the features with '0' weights and lesser weights which are unimportant for the prediction.The scores of linear regression and lasso are almost same that means the model is  overfit."""